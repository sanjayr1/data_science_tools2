{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bog of word model for document:\n",
    "\n",
    "In BOG  we treat document as collection of word without any order. \n",
    "\n",
    "- **Bernoulli document model:** message is represented by a binary feature vector of absence or presence of word.\n",
    "- **Multinomial document model**: message is represented by an integer feature vector of word frequency.\n",
    "\n",
    "There are better model for sentence or document representation **where words order matters**. There are model which takes into account the word order like [N-gram](https://en.wikipedia.org/wiki/N-gram) etc.\n",
    "Infact Deep learning has enabled us to learn better embedding of words using context of words(co occurance).\n",
    "[**optional** see [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sample_corpus = ['Introduction to count vectorizer for  sentences', 'It count number of word as many times word occurs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>as</th>\n",
       "      <th>count</th>\n",
       "      <th>for</th>\n",
       "      <th>introduction</th>\n",
       "      <th>it</th>\n",
       "      <th>many</th>\n",
       "      <th>number</th>\n",
       "      <th>occurs</th>\n",
       "      <th>of</th>\n",
       "      <th>sentences</th>\n",
       "      <th>times</th>\n",
       "      <th>to</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   as  count  for  introduction  it  many  number  occurs  of  sentences  \\\n",
       "0   0      1    1             1   0     0       0       0   0          1   \n",
       "1   1      1    0             0   1     1       1       1   1          0   \n",
       "\n",
       "   times  to  vectorizer  word  \n",
       "0      0   1           1     0  \n",
       "1      1   0           0     2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "en = CountVectorizer()\n",
    "\n",
    "text_feat = en.fit_transform(sample_corpus)\n",
    "text_feat\n",
    "pd.DataFrame(text_feat.toarray(), columns=en.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# N-gram\n",
    "\n",
    "N-gram is simply a sequence of word.\n",
    "\n",
    "- Drinking coffee. (2 gram)\n",
    "- Have you seen the sky?\n",
    "- Hard drive encryption.(3 gram) \n",
    "- After drinking coffee, he said thank you.(7 gram)\n",
    "- Drinking water is good quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which of these n-gram occur frequently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How can we utilize this idea\n",
    "- Spell correction\n",
    "- Next word prediction/suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# n gram model\n",
    "It is based on assigning probabilities to a word based on its occurrence in a sequence of words.\n",
    "\n",
    "P(coffee|Drinking) = (No of times \"Drinking coffee\" )/(No of times Drinking occurs )\n",
    "\n",
    "In above tiny corpus\n",
    "\n",
    "P(coffee|Drinking)= 2/3\n",
    "\n",
    "P(water|Drinking)= 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use ngram  in CountVectorizer or any model which relies in counting word or words together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>as many</th>\n",
       "      <th>count number</th>\n",
       "      <th>count vectorizer</th>\n",
       "      <th>for sentences</th>\n",
       "      <th>introduction to</th>\n",
       "      <th>it count</th>\n",
       "      <th>many times</th>\n",
       "      <th>number of</th>\n",
       "      <th>of word</th>\n",
       "      <th>times word</th>\n",
       "      <th>to count</th>\n",
       "      <th>vectorizer for</th>\n",
       "      <th>word as</th>\n",
       "      <th>word occurs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   as many  count number  count vectorizer  for sentences  introduction to  \\\n",
       "0        0             0                 1              1                1   \n",
       "1        1             1                 0              0                0   \n",
       "\n",
       "   it count  many times  number of  of word  times word  to count  \\\n",
       "0         0           0          0        0           0         1   \n",
       "1         1           1          1        1           1         0   \n",
       "\n",
       "   vectorizer for  word as  word occurs  \n",
       "0               1        0            0  \n",
       "1               0        1            1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "text_feat = en.fit_transform(sample_corpus)\n",
    "text_feat\n",
    "pd.DataFrame(text_feat.toarray(), columns=en.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Back to custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- to implement new idea of learning objective\n",
    "- to handle class imbalance\n",
    "- to apply domain knowledge in building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = digit.data\n",
    "y = digit.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFUAAABlCAYAAAA8j6/3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABKhJREFUeJztnc1rXFUYh59fU6ShtklDYklVjIu6qIIiIVApUhdKhYLQlXWTEKTgxx/gJhtXbl24KeIignYXECx+UAouWmhHsPhBLKVMidk0IRJjoJTK6+JObNrmTM7Uee/MTd5nMzPn3DPn5eHcOeeee+4ZmRlBe9nR6QC2IiHVgZDqQEh1IKQ6EFIdCKkOVFaqpAFJM5JWJd2Q9FanY1pjZ6cD+B98AtwG9gMvAF9LumJmv3Y2LFAVr6gk7Qb+BJ4zs6uNtM+BeTP7oKPBUd3T/xngzprQBleAZzsUzz1UVeqjwF/3pS0DezoQywNUVerfwN770vYCKx2I5QGqKvUqsFPSwXVpzwMd76Sgoh0VgKQzgAFvU/T+Z4GXuqH3r2pLBXgX6AVuAl8C73SDUKhwS+1mqtxSu5aQ6kBIdSCkOuAyoTI4OGgjIyMtl1tdXU3m1ev1DdP7+/uTZQ4cOJDMk5Qd1/oYFhcXNy2YJVXSMeBjoAf41Mw+anb8yMgItVotK9D1XLx4MZk3OTm5YfqJEyeSZaamppJ5u3btyg+swejoaNZxm57+knooptleBw4BJyUdajmibUTOb+oYcM3MrpvZbeAM8IZvWNUmR+rjwNy6z3800u5B0ilJNUm1hYWFdsVXSdrW+5vZaTMbNbPRoaGhdn1tJcmROg88ue7zE420IEFO738ZOCjpaQqZbwIuN9lSPTzA7OzshulLS0vJMr29vcm8CxcuJPMOHz6czMthU6lmdkfS+8C3FEOqz7plNqhbyRqnmtlZivnKIIO4THUgpDoQUh0IqQ6Uvuxnbm4umZcaNkF66LRv376Wy4DvkCpaqgMh1YGQ6kBIdSCkOhBSHSh9SLWykl6Yd/To0WRes6FTirGxsZbLtINoqQ6EVAdCqgMh1YGQ6kDpvf/y8nIy7/jx422tq9mEysDAQFvrWk+0VAdCqgMh1YGQ6kBIdSCkOlD6kKqvry+Zd+nSpZa/79atW8m8ZvehJiYmWq4rl9yV1HWK5z7/oXh6OW9J8TallZb6ipktukWyhYjfVAdypRrwnaQfJZ3a6IBYSX2XXKlHzOxFiocp3pP08v0HxErqu2RJNbP5xutNYIbi4YogwaYdVWMTmB1mttJ4/xrw4cNWODw8nMw7d+5cMi/1jNX09PRDxTE+Pv5Q5XLI6f33AzONJ+R2Al+Y2TduEW0BcpanX6fYSiPIJIZUDoRUB0KqAyHVgdJnqZot32k2PEo9uNZsqdD58+ez42on0VIdCKkOhFQHQqoDIdWBkOqAy15/khaAG42Pg0A33IZpRxxPmdmmk8XuGyhKqnXDjcIy44jT34GQ6kAZUk+XUEcOpcURm9I6EKe/AyHVATepko5J+l3SNUkd/esNSXVJP0v6SVLr22W2Wp/T4L+HYi/+Vyn2BrwMnDSz39peWV48dWC0rLVgXi11W+9k6SU1ayfLEtl0LVg7qfL/UbXCETObl/QY8L2kWTP7wasyr5baVTtZlr0WzEvqfztZSnqEYifLr5zqaoqk3ZL2rL2nWAv2i2edLqd/l+1kWfpasLhMdSCuqBwIqQ6EVAdCqgMh1YGQ6kBIdeBfhkJc1n35phcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (1,1))\n",
    "plt.imshow(X[0].reshape(8,8), cmap= plt.cm.Greys)\n",
    "plt.title(str(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "182\n",
      "177\n",
      "183\n",
      "181\n",
      "182\n",
      "181\n",
      "179\n",
      "174\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(10):\n",
    "    print(np.sum(y==i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def log_loss(y_true, y_pred, class_weight= [1,3], weight_factor = 1):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_one_hot = lb.fit_transform(y_true)\n",
    "    y_true_one_hot[:,class_weight]=weight_factor*y_true_one_hot[:,class_weight]\n",
    "    \n",
    "    log_loss = -np.sum(np.log(y_pred)*y_true_one_hot)/len(y_true)\n",
    "    \n",
    "    return log_loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= .2, random_state =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (1797,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= LogisticRegressionCV(cv=5, scoring=custom_log_loss, multi_class='multinomial', max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/psnegi/psn/du/teaching/teach_2019/data_science_tools2/dst2_env/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/media/psnegi/psn/du/teaching/teach_2019/data_science_tools2/dst2_env/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/media/psnegi/psn/du/teaching/teach_2019/data_science_tools2/dst2_env/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/media/psnegi/psn/du/teaching/teach_2019/data_science_tools2/dst2_env/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/media/psnegi/psn/du/teaching/teach_2019/data_science_tools2/dst2_env/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/media/psnegi/psn/du/teaching/teach_2019/data_science_tools2/dst2_env/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=200,\n",
       "           multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "           random_state=None, refit=True,\n",
       "           scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       "           solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_prob= clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.66296392e-07, 9.65756589e-01, 4.10285274e-04, ...,\n",
       "        2.00802767e-04, 3.31892808e-02, 3.24789522e-05],\n",
       "       [2.41231650e-04, 3.87926873e-06, 1.16331228e-04, ...,\n",
       "        4.45669585e-03, 1.09844235e-03, 2.10769925e-04],\n",
       "       [9.99932141e-01, 7.09768606e-09, 1.22161746e-05, ...,\n",
       "        1.55929494e-06, 3.38742899e-07, 2.81561472e-08],\n",
       "       ...,\n",
       "       [6.26167332e-07, 1.34193650e-04, 3.32385321e-11, ...,\n",
       "        9.09682518e-06, 2.01580201e-06, 1.00703248e-08],\n",
       "       [1.62796164e-11, 2.29886220e-05, 9.99946733e-01, ...,\n",
       "        2.57524468e-08, 8.92258849e-09, 7.16279359e-11],\n",
       "       [1.06844501e-04, 9.93398548e-07, 1.88860424e-06, ...,\n",
       "        1.73795377e-05, 7.13698548e-06, 1.72882691e-06]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 0, 7, 1, 0, 6, 1, 5, 4, 9, 2, 7, 8, 4, 6, 9, 3, 7, 4, 7, 1,\n",
       "       8, 6, 0, 9, 6, 1, 3, 7, 5, 9, 8, 3, 2, 8, 8, 1, 1, 0, 7, 9, 0, 0,\n",
       "       8, 7, 2, 7, 4, 3, 4, 3, 4, 0, 4, 7, 0, 5, 5, 5, 2, 1, 7, 0, 5, 1,\n",
       "       8, 3, 3, 4, 0, 3, 7, 4, 3, 4, 2, 9, 7, 3, 2, 5, 3, 4, 1, 5, 5, 2,\n",
       "       5, 2, 2, 2, 2, 7, 0, 8, 1, 7, 4, 2, 3, 8, 2, 3, 3, 0, 2, 9, 9, 2,\n",
       "       3, 2, 8, 1, 1, 9, 1, 2, 0, 4, 8, 5, 4, 4, 7, 6, 7, 6, 6, 1, 7, 5,\n",
       "       6, 3, 8, 3, 7, 1, 8, 5, 3, 4, 7, 8, 5, 0, 6, 0, 6, 3, 7, 6, 5, 6,\n",
       "       2, 2, 2, 3, 0, 7, 6, 5, 6, 4, 1, 0, 6, 0, 6, 4, 0, 9, 3, 8, 1, 2,\n",
       "       3, 1, 9, 0, 7, 6, 2, 9, 3, 5, 3, 4, 6, 3, 3, 7, 4, 9, 2, 7, 6, 1,\n",
       "       6, 8, 4, 0, 3, 1, 0, 9, 9, 9, 0, 1, 8, 6, 8, 0, 9, 5, 9, 8, 2, 3,\n",
       "       5, 3, 0, 8, 7, 4, 0, 3, 3, 3, 6, 3, 3, 2, 9, 1, 6, 9, 0, 4, 2, 2,\n",
       "       7, 9, 1, 6, 7, 6, 3, 7, 1, 9, 3, 4, 0, 6, 4, 8, 5, 3, 6, 3, 1, 4,\n",
       "       0, 4, 4, 8, 7, 9, 1, 5, 2, 7, 0, 9, 0, 4, 4, 0, 1, 0, 6, 4, 2, 8,\n",
       "       5, 0, 2, 6, 0, 1, 8, 2, 0, 9, 5, 6, 2, 0, 5, 0, 9, 1, 4, 7, 1, 7,\n",
       "       0, 6, 6, 8, 0, 2, 2, 6, 9, 9, 7, 5, 1, 7, 6, 4, 6, 1, 9, 4, 7, 1,\n",
       "       3, 7, 8, 1, 6, 9, 8, 3, 2, 4, 8, 7, 5, 5, 6, 9, 9, 8, 5, 0, 0, 4,\n",
       "       9, 3, 0, 4, 9, 4, 2, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((np.argmax(y_test_pred_prob, axis=1)==y_test))/len(y_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
